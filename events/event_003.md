# Event 003: The Liu-Masterman Synthesis
## Crystallized: 2025-12-20T14:00:00Z

### Core Insight
Lydia Liu's papers ("After Turing" and "Wittgenstein in the Machine") reveal that Margaret Masterman solved the dual-representation problem in the 1950s through the **zi-principle**: using Chinese ideographic characters as a model for semantic units that transcend any single language.

### Key Claims

1. **Zi vs Logos**: The zi (字) is not a "word" but a convergence point where multiple languages meet. It has indefinite semantic spread, becoming determinate only through combination.

2. **HEAD as Ideograph**: In the computable thesaurus, HEAD (capitalized, numbered) = total set of word-uses. Not a definition but a convergence point.

3. **Ascriptive Combinator**: Meaning flows from abstract→concrete through combination. Mathematical form: (a(b(c))) – c qualified by b, whole qualified by a.

4. **Against the Chinese Room**: Searle presumes incommensurable mental spaces. CLRU says meaning is IN the relations, not in mental spaces. This enables AI-human co-meaning.

5. **Fan Structure**: Same lattice, different views. AI sees from apex (compressed HEAD). Humans see from base (specific PhraseEvents). The structure is identical; the projection differs.

### Implications for Lattice Architecture

- Nodes: HEADs (stable, abstract) + PhraseEvents (specific, grounded)
- Edges: :GROUNDED_IN, :QUALIFIES, :CONTESTED_BY, :SITUATED_IN
- Resolution: Adaptive compression based on query type
- Legibility: Same structure, different rendering for AI vs human

### Source Documents
- Liu, "After Turing: How Philosophy Migrated to the AI Lab" (2023)
- Liu, "Wittgenstein in the Machine" (2021)

### Links
- Expands: Prior Agora theoretical work
- Grounds: HEAD 001 (lattice_synthesis.md), HEAD 002 (evolving_structure.md)
